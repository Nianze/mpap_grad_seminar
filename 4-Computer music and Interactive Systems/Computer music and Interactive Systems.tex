\documentclass[jou]{apa6}

\usepackage[american]{babel}

\usepackage{csquotes}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{bibliography.bib}

\title{Music Composition Assisted by Machine Learning}

\author{Nianze Liu}
\affiliation{New York University}

\leftheader{Liu}

\abstract{This article overviews recent progress in machine learning as a creative assistance in music production. Specifically, two innovative machine learning models, NSynth and Coconet, are introduced and dicussed in the context of music composition, music production, and music live performance.}

\keywords{Computer Music, Machine Learning}

\begin{document}
\maketitle

\section{Introduction}

Since the invention of computer, music industry has hugely been influenced. Nowadays, we can find a huge number of music pieces in the digital format: some might be played by acoustic instruments and recorded down, others may be generated by synthesizers, but no matter how the sound is generated, the music waves ends up into a sequence of digital representations, which turns out to be a good data source for machine learning related applications. With the hardware becomes more and more powerful in the recent years, more complicated machine learning models become feasible for providing helpful assistances in various areas, such as automatic driving, speech recognition, and live translation. Music production is no exception. By the help of new machine learning models, we might train the computer to learn the music master's composition techniques to help us generate harmonies given a piece of melody we wrote; it is also possible to synthesize new sound that might be very difficult to create by the old synthesizer-based motheds.

In this article, two machine learning models are discussed. We will go over their basic structures to see how they work. After understanding the abilities of these innovative models, we will discuss what might be changed by introducing these techniques into the current music composition, music production and even live music performance, and what might be achieved in the future.

\section{Computer Assisted Composition}

\subsection{How Computer Generates Music Traditionally}

Traditionally, when computer generates music, the most natural way is to do the job in a sequence, under an orderly manner, because when computer generates music, the only thing it understands is just numbers, or in our context,  statistics and probability - it does not feel. Given a note, such as C4, and another note, say E4, the computer has to calculate all the possibilities of the next note, and return the most possible notes to fill in the next position. If  the model sees millions of times that C4, E4 and G4 always show together during training, the likelihood of generating a G4 given C4 and E4 is then slightly higher -- and we can say that the model ‘learns’ the ‘triad’ now, even though the computer knows nothing about the concept of ‘triad’, it only knows that G4 is most likely to show up in the context of C4 and E4. (After all, C Major triad is formed only because human’s perception decides the sound feels good). Thus, before determining the very first note of  music, the computer has to have a complete view of  the whole piece, since as long as the previous note is determined, there’s no way back to refine it anymore -- the note has to be generated in a sequence in one go. This orderly generation requirement is very strict,  which leads to the machine learning model behind the Google Doodle online interactive game: ‘Coconet’.

\subsection{Coconet}

The machine learning model Coconet is basically a convolutional neural network but is integrated with some creative process. The details of the model are described in Huang, C A’s paper “Counterpoint by convolution”. Here we’ll just go over the basics of the model structure and see how it works to generate four voices counter points and harmonize any given input melody.
The biggest idea behind the model is to mimic the process of human composition. The key observation here is that, when human beings compose, the composer will seldom write the music in sequence: composer tends to rewrite the same piece back and forth over and over again in order to get the better refined result that finally meets his or her standard. In other words, the composition is typically done in an orderless manner. To achieve orderless composition by computer, two techniques is created: Orderless modeling & Gibbs sampling.

The basic idea is to use Gibbs sampling to generate music in multiple iterations, and in every iteration part of music scores is erased, so that we can use Orderless modeling to come up with the independent conditional probabilities for missing notes. For the same example of C4 E4 G4 triad,  if we mask the E4 in the middle, the model should be able to generate E4 given the context of  C4 in the first beat and G4 in the 3rd beat. In math, the model is basically computing: 

P(x2|x1, x3), where xn means score at beat n 

Gibbs sampling is just a fancy word for phrasing our repeated iterative music generation/modification in multiple passes, and orderless modeling guarantees that no matter which note is randomly erased by a mask in a random iteration, the model is able to generate a conditional distribution for the missing score.

The outcome of Coconet is very interesting: given a partial piece of scores for a fixed length, the model is able to complete the music based on how it gets trained. In the Google Doodle example, the model learns from “a dataset of 306 chorale harmonizations by Bach”, according to Magenta Blog post. This interactive game is still available to play today by accessing this link: https://www.google.com/doodles/celebrating-johann-sebastian-bach.


\section{Computer Assisted Systhesis}


\section{Future Possibilities}


\printbibliography

\end{document}